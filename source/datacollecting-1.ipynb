{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"| **ATTRIBUTES**            |**MEANING**               |\n|:----------------------|:-------------------------------------------------------------|\n|**`Title`**            | Title of the manga (written in English phonetic)                                                |\n|**`Score`**            | Score on the MyAnimeList site (MAL)                                                             |\n|**`Vote`**             | Number of readers voting for the manga                                                          |\n|**`Ranked`**           | Ranking of manga on the web MyAnimeList (MAL)                                                   |\n|**`Popularity`**       | The popularity of the manga                                                                     |\n|**`Members`**          | Number of readers who have this manga in their list                                             |\n|**`Favorite`**         | Number of readers who love this manga                                                           |\n|**`Type`**\t\t        | Type (manga/manhwa/lightnovel...)                                                               |\n|**`Volumes`**          | Number of volumes of manga                                                                      |\n|**`Chapters`**         | Number of chapters of manga                                                                     |\n|**`Status`**           | Status of the manga (ongoing, completed, on hiatus,...)                                         |\n|**`Published`**        | Release time to the end time of the manga                                                       |\n|**`Genres`**           | Genres of manga                                                                                 |\n|**`Themes`**           | The themes of the manga                                                                         |\n|**`Demographics`** \t| Target demographic (e.g., Shounen).                                                             |\n|**`Serialization`** \t| Manga serialization information (e.g., Shounen Jump).                                           |\n|**`Author`**           | Author of manga                                                                                 |\n|**`Total Review`**     | Number of readers leaving comments on the manga                                                 |\n|**`Type Review`**      | Number of readers for each comment category (Recommended / Mixed feeling / Not recommended)     | / Not recommended)","metadata":{}},{"cell_type":"code","source":"!pip install requests-html","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T07:48:40.801699Z","iopub.execute_input":"2024-11-17T07:48:40.802169Z","iopub.status.idle":"2024-11-17T07:48:58.319272Z","shell.execute_reply.started":"2024-11-17T07:48:40.802123Z","shell.execute_reply":"2024-11-17T07:48:58.318039Z"}},"outputs":[{"name":"stdout","text":"Collecting requests-html\n  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from requests-html) (2.32.3)\nCollecting pyquery (from requests-html)\n  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\nCollecting fake-useragent (from requests-html)\n  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\nCollecting parse (from requests-html)\n  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting bs4 (from requests-html)\n  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\nCollecting w3lib (from requests-html)\n  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\nCollecting pyppeteer>=0.0.14 (from requests-html)\n  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\nRequirement already satisfied: certifi>=2023 in /opt/conda/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html) (2024.8.30)\nRequirement already satisfied: importlib-metadata>=1.4 in /opt/conda/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html) (7.0.0)\nCollecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html) (4.66.4)\nRequirement already satisfied: urllib3<2.0.0,>=1.25.8 in /opt/conda/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.18)\nCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from bs4->requests-html) (4.12.3)\nRequirement already satisfied: lxml>=2.1 in /opt/conda/lib/python3.10/site-packages (from pyquery->requests-html) (5.3.0)\nCollecting cssselect>=1.2.0 (from pyquery->requests-html)\n  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->requests-html) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->requests-html) (3.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.19.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->bs4->requests-html) (2.5)\nDownloading requests_html-0.10.0-py3-none-any.whl (13 kB)\nDownloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\nDownloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\nDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\nDownloading pyquery-2.0.1-py3-none-any.whl (22 kB)\nDownloading w3lib-2.2.1-py3-none-any.whl (21 kB)\nDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\nDownloading pyee-11.1.1-py3-none-any.whl (15 kB)\nDownloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: parse, fake-useragent, websockets, w3lib, pyee, cssselect, pyquery, pyppeteer, bs4, requests-html\n  Attempting uninstall: websockets\n    Found existing installation: websockets 12.0\n    Uninstalling websockets-12.0:\n      Successfully uninstalled websockets-12.0\nSuccessfully installed bs4-0.0.2 cssselect-1.2.0 fake-useragent-1.5.1 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 w3lib-2.2.1 websockets-10.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install lxml_html_clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T07:48:58.322193Z","iopub.execute_input":"2024-11-17T07:48:58.322728Z","iopub.status.idle":"2024-11-17T07:49:12.443001Z","shell.execute_reply.started":"2024-11-17T07:48:58.322663Z","shell.execute_reply":"2024-11-17T07:49:12.441589Z"}},"outputs":[{"name":"stdout","text":"Collecting lxml_html_clean\n  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from lxml_html_clean) (5.3.0)\nDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\nInstalling collected packages: lxml_html_clean\nSuccessfully installed lxml_html_clean-0.4.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import requests\nfrom requests_html import HTMLSession\nfrom bs4 import BeautifulSoup\nimport re\nimport nest_asyncio\nimport pandas as pd \nimport datetime\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T07:49:12.444769Z","iopub.execute_input":"2024-11-17T07:49:12.445318Z","iopub.status.idle":"2024-11-17T07:49:13.285435Z","shell.execute_reply.started":"2024-11-17T07:49:12.445271Z","shell.execute_reply":"2024-11-17T07:49:13.284279Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"nest_asyncio.apply() # Cho phép một vòng lặp sự kiện đang chạy chấp nhận các vòng lặp con.\nsession = HTMLSession() # Render JavaScript hoặc xử lý nội dung động của trang web (comment...)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T07:49:13.288408Z","iopub.execute_input":"2024-11-17T07:49:13.289003Z","iopub.status.idle":"2024-11-17T07:49:13.294773Z","shell.execute_reply.started":"2024-11-17T07:49:13.288958Z","shell.execute_reply":"2024-11-17T07:49:13.293751Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Crawl URLS","metadata":{}},{"cell_type":"code","source":"listUrl1 = []\n\nfor i in range(0, 5000, 50):\n    # Url of the website to scrap\n    url = f'https://myanimelist.net/topmanga.php?limit={i}'\n\n    # Get the html content\n    html = requests.get(url).text\n\n    # Parse the html content\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    # Get the list of manga\n    listItem = soup.find_all(\"td\", {\"class\": \"title al va-t clearfix word-break\"})\n\n    # Get the url of each manga\n    for item in listItem:\n        listUrl1.append(item.find('a').get('href'))\n\n    # Print the number of manga urls collected\n    print(f'{len(listUrl1)} urls collected', end='\\r', flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T07:49:13.296332Z","iopub.execute_input":"2024-11-17T07:49:13.296778Z","iopub.status.idle":"2024-11-17T07:49:57.544054Z","shell.execute_reply.started":"2024-11-17T07:49:13.296727Z","shell.execute_reply":"2024-11-17T07:49:57.542720Z"}},"outputs":[{"name":"stdout","text":"5000 urls collected\r","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Concatenate list URLs","metadata":{}},{"cell_type":"code","source":"listUrl = listUrl1\nprint(f'Total: {len(listUrl)} urls collected')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T07:49:57.545447Z","iopub.execute_input":"2024-11-17T07:49:57.545802Z","iopub.status.idle":"2024-11-17T07:49:57.551580Z","shell.execute_reply.started":"2024-11-17T07:49:57.545763Z","shell.execute_reply":"2024-11-17T07:49:57.550183Z"}},"outputs":[{"name":"stdout","text":"Total: 5000 urls collected\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"with open(\"/kaggle/working/link_collecting_1.txt\", \"w\") as file:\n    file.writelines(item + \"\\n\" for item in listUrl1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T07:49:57.553081Z","iopub.execute_input":"2024-11-17T07:49:57.553463Z","iopub.status.idle":"2024-11-17T07:49:57.567358Z","shell.execute_reply.started":"2024-11-17T07:49:57.553425Z","shell.execute_reply":"2024-11-17T07:49:57.566103Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"collect_data\"></a>\n\n## <span style='color:#2B9C15 '> 📕 Collect data of each manga  </span>\n1. From each url collected above, send a GET request to get the HTML content of the page.\n2. If length of the HTML content is smaller than 4000 , sleep for 10 seconds and send the GET request again. Because that means the website has blocked the connection and we need to wait for a while before sending the request again.\n3. Save the HTML content in a list for parsing later.\n\nThis process still splits into 2 parts, each part collects 5000 HTML contents to avoid the connection being interrupted by the website due to too many requests.","metadata":{}},{"cell_type":"markdown","source":"### 👉 Crawl HTML content from the 20000 manga/light novel/... URLs","metadata":{}},{"cell_type":"code","source":"listHtml1 = []\n\nfor url in listUrl[0:5000]:\n    res = session.get(url)\n    while len(res.text) < 4000:\n        # Sleep for 10 minutes\n        time.sleep(200)\n        res = session.get(url)\n        \n    listHtml1.append(res.text)\n\n    # Print the number of manga html collected\n    print(f'{len(listHtml1)}/{len(listUrl)} manga html collected', end='\\r', flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T07:49:57.568827Z","iopub.execute_input":"2024-11-17T07:49:57.569351Z","iopub.status.idle":"2024-11-17T09:02:03.729626Z","shell.execute_reply.started":"2024-11-17T07:49:57.569310Z","shell.execute_reply":"2024-11-17T09:02:03.728489Z"}},"outputs":[{"name":"stdout","text":"5000/5000 manga html collected\r","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Extract time of data collection to report for the project\nnow = datetime.datetime.now()\nnow = now.strftime(\"%Y-%m-%d\")\nprint(\"Time of data collection: \", now)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T09:02:03.731045Z","iopub.execute_input":"2024-11-17T09:02:03.731433Z","iopub.status.idle":"2024-11-17T09:02:03.738062Z","shell.execute_reply.started":"2024-11-17T09:02:03.731393Z","shell.execute_reply":"2024-11-17T09:02:03.736828Z"}},"outputs":[{"name":"stdout","text":"Time of data collection:  2024-11-17\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"listHtml = listHtml1\nprint(f'Total: {len(listHtml)} manga html collected')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T09:02:03.741284Z","iopub.execute_input":"2024-11-17T09:02:03.741684Z","iopub.status.idle":"2024-11-17T09:02:03.755245Z","shell.execute_reply.started":"2024-11-17T09:02:03.741644Z","shell.execute_reply":"2024-11-17T09:02:03.754207Z"}},"outputs":[{"name":"stdout","text":"Total: 5000 manga html collected\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### 👉 Extracting the detailed values of each comic website page","metadata":{}},{"cell_type":"code","source":"def extract_info(htmlComic):\n    soup = BeautifulSoup(htmlComic, \"html.parser\")\n\n    title = soup.find('span', {'itemprop': 'name'})\n    if title is None:\n        return None\n    else:\n        title_text = title.text.strip()\n        title_english_span = title.find('span', {'class': 'title-english'})\n\n        if title_english_span is not None:\n            title_english_text = title_english_span.text.strip()\n            title_text = title_text.replace(title_english_text, '')\n            title = f'{title_text} ({title_english_text})'\n        else:\n            title = title_text\n    \n    ratingValue = soup.find('span', {'itemprop': 'ratingValue'}).text\n    ratingCount = soup.find('span', {'itemprop': 'ratingCount'}).text\n    ranked = re.findall(r'\\d+', soup.find('span', {'class': 'numbers ranked'}).text)[0]\n    popularity = re.findall(r'\\d+', soup.find('span', {'class': 'numbers popularity'}).text)[0]\n\n    volumes, chapters, status, published = '', '', '', ''\n    genres, themes, authors, favorites, members = [], [], '', '', ''\n    type_, demographic, serialization = '', '', ''\n\n    for space in soup.find_all(\"div\", {'class': 'spaceit_pad'}):\n        text = space.text.strip()\n        \n        if 'Type:' in text:\n            type_ = text.split(':', 1)[1].strip()\n        elif 'Volumes:' in text:\n            volumes = text.split(':', 1)[1].strip()\n        elif 'Chapters:' in text:\n            chapters = text.split(':', 1)[1].strip()\n        elif 'Status:' in text:\n            # Lấy nội dung sau thẻ <span class=\"dark_text\">\n            status = space.find('span', {'class': 'dark_text'}).next_sibling.strip()\n        elif 'Published:' in text:\n            published = text.split(':', 1)[1].strip()\n        elif 'Genres:' in text or 'Genre:' in text:\n            genres = [gen.text.strip() for gen in space.find_all('a')]\n        elif 'Themes:' in text or 'Theme:' in text:\n            # Lấy cả giá trị từ <a> và <span itemprop=\"genre\">\n            themes = [theme.text.strip() for theme in space.find_all('a')]\n        elif 'Demographic:' in text or 'Demographics:' in text:\n            demographic = space.find('a').text.strip()\n        elif 'Serialization:' in text or 'Serializations:' in text:\n            # serialization = space.find('a').text.strip()\n            serialization_tag = space.find('a')  # Tìm thẻ <a>\n            serialization = serialization_tag.text.strip() if serialization_tag else ''  # Kiểm tra nếu không tìm thấy\n        elif 'Authors:' in text or 'Author:' in text:\n            authors = text.split(':')[1].strip()\n            # authors = space.find('a').text.strip()\n            # author_tag = space.find('a')  # Tìm thẻ <a>\n            # authors = author_tag.text.strip() if author_tag else ''  # Kiểm tra nếu không tìm thấy\n        elif 'Favorites:' in text:\n            favorites = text.split(':', 1)[1].strip()\n        elif 'Members:' in text:\n            members = text.split(':', 1)[1].strip()\n\n    infoReviews = soup.find('div', {'class': 'manga-info-review__header mal-navbar'})\n    totalReviews = re.findall(r'\\d+', infoReviews.find('div', {'class': 'right'}).text)[0]\n\n    typeReview = [\n        int(re.findall(r'\\d+', infoReviews.find('div', {'class': 'recommended'}).text)[0]),\n        int(re.findall(r'\\d+', infoReviews.find('div', {'class': 'mixed-feelings'}).text)[0]),\n        int(re.findall(r'\\d+', infoReviews.find('div', {'class': 'not-recommended'}).text)[0])\n    ]\n\n    return {\n        \"Title\": title, \"Score\": ratingValue, \"Vote\": ratingCount,\n        \"Ranked\": ranked, \"Popularity\": popularity, \"Members\": members,\n        \"Favorite\": favorites, \"Types\": type_, \"Volumes\": volumes, \n        \"Chapters\": chapters, \"Status\": status, \"Published\": published, \n        \"Genres\": genres, \"Themes\": themes, \"Demographic\": demographic, \"Serialization\": serialization, \n        \"Author\": authors, \"Total Review\": totalReviews, \"Type Review\": typeReview\n    }\n\n# data_list = [extract_info(htmlComic) for htmlComic in listHtml if extract_info(htmlComic) is not None]\n# df = pd.DataFrame(data_list)\ndata_list = []\nfor idx, htmlComic in enumerate(listHtml, start=1):\n    result = extract_info(htmlComic)\n    if result is not None:\n        data_list.append(result)\n    # In trạng thái sau khi duyệt mỗi phần tử\n    print(f\"Đã xử lý {idx}/{len(listHtml)} phần tử.\", end='\\r', flush=True)\n    # print(f'{len(listUrl1)} urls collected', end='\\r', flush=True)\n\ndf = pd.DataFrame(data_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:06:22.374100Z","iopub.execute_input":"2024-11-17T10:06:22.374600Z","iopub.status.idle":"2024-11-17T10:16:05.045399Z","shell.execute_reply.started":"2024-11-17T10:06:22.374549Z","shell.execute_reply":"2024-11-17T10:16:05.044397Z"}},"outputs":[{"name":"stdout","text":"Đã xử lý 5000/5000 phần tử.\r","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:19:52.066544Z","iopub.execute_input":"2024-11-17T10:19:52.066981Z","iopub.status.idle":"2024-11-17T10:19:52.095609Z","shell.execute_reply.started":"2024-11-17T10:19:52.066939Z","shell.execute_reply":"2024-11-17T10:19:52.094358Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                               Title Score    Vote Ranked  \\\n0                                            Berserk  9.47  363720      1   \n1  JoJo no Kimyou na Bouken Part 7: Steel Ball Ru...  9.31  172219      2   \n2                                           Vagabond  9.26  154583      3   \n3                                          One Piece  9.22  392811      4   \n4                                            Monster  9.16  104327      5   \n\n  Popularity  Members Favorite  Types  Volumes Chapters      Status  \\\n0          1  725,079  130,489  Manga  Unknown  Unknown  Publishing   \n1         23  280,428   46,269  Manga       24       96    Finished   \n2         13  406,082   44,258  Manga       37      327   On Hiatus   \n3          4  642,620  119,974  Manga  Unknown  Unknown  Publishing   \n4         29  258,581   22,008  Manga       18      162    Finished   \n\n                        Published  \\\n0              Aug  25, 1989 to ?   \n1  Jan  19, 2004 to Apr  19, 2011   \n2   Sep  3, 1998 to May  21, 2015   \n3              Jul  22, 1997 to ?   \n4   Dec  5, 1994 to Dec  20, 2001   \n\n                                              Genres  \\\n0  [Action, Adventure, Award Winning, Drama, Fant...   \n1         [Action, Adventure, Mystery, Supernatural]   \n2                 [Action, Adventure, Award Winning]   \n3                       [Action, Adventure, Fantasy]   \n4                    [Award Winning, Drama, Mystery]   \n\n                                       Themes Demographic  \\\n0  [Gore, Military, Mythology, Psychological]      Seinen   \n1                                [Historical]      Seinen   \n2                       [Historical, Samurai]      Seinen   \n3                                          []     Shounen   \n4                 [Adult Cast, Psychological]      Seinen   \n\n           Serialization                                             Author  \\\n0           Young Animal   Miura, Kentarou (Story & Art), Studio Gaga (Art)   \n1             Ultra Jump                      Araki, Hirohiko (Story & Art)   \n2                Morning  Inoue, Takehiko (Story & Art), Yoshikawa, Eiji...   \n3  Shounen Jump (Weekly)                        Oda, Eiichiro (Story & Art)   \n4     Big Comic Original                       Urasawa, Naoki (Story & Art)   \n\n  Total Review    Type Review  \n0          289  [252, 17, 20]  \n1          131    [123, 7, 1]  \n2          104     [93, 9, 2]  \n3          231  [190, 21, 20]  \n4           86    [69, 11, 6]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Score</th>\n      <th>Vote</th>\n      <th>Ranked</th>\n      <th>Popularity</th>\n      <th>Members</th>\n      <th>Favorite</th>\n      <th>Types</th>\n      <th>Volumes</th>\n      <th>Chapters</th>\n      <th>Status</th>\n      <th>Published</th>\n      <th>Genres</th>\n      <th>Themes</th>\n      <th>Demographic</th>\n      <th>Serialization</th>\n      <th>Author</th>\n      <th>Total Review</th>\n      <th>Type Review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Berserk</td>\n      <td>9.47</td>\n      <td>363720</td>\n      <td>1</td>\n      <td>1</td>\n      <td>725,079</td>\n      <td>130,489</td>\n      <td>Manga</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Publishing</td>\n      <td>Aug  25, 1989 to ?</td>\n      <td>[Action, Adventure, Award Winning, Drama, Fant...</td>\n      <td>[Gore, Military, Mythology, Psychological]</td>\n      <td>Seinen</td>\n      <td>Young Animal</td>\n      <td>Miura, Kentarou (Story &amp; Art), Studio Gaga (Art)</td>\n      <td>289</td>\n      <td>[252, 17, 20]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>JoJo no Kimyou na Bouken Part 7: Steel Ball Ru...</td>\n      <td>9.31</td>\n      <td>172219</td>\n      <td>2</td>\n      <td>23</td>\n      <td>280,428</td>\n      <td>46,269</td>\n      <td>Manga</td>\n      <td>24</td>\n      <td>96</td>\n      <td>Finished</td>\n      <td>Jan  19, 2004 to Apr  19, 2011</td>\n      <td>[Action, Adventure, Mystery, Supernatural]</td>\n      <td>[Historical]</td>\n      <td>Seinen</td>\n      <td>Ultra Jump</td>\n      <td>Araki, Hirohiko (Story &amp; Art)</td>\n      <td>131</td>\n      <td>[123, 7, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Vagabond</td>\n      <td>9.26</td>\n      <td>154583</td>\n      <td>3</td>\n      <td>13</td>\n      <td>406,082</td>\n      <td>44,258</td>\n      <td>Manga</td>\n      <td>37</td>\n      <td>327</td>\n      <td>On Hiatus</td>\n      <td>Sep  3, 1998 to May  21, 2015</td>\n      <td>[Action, Adventure, Award Winning]</td>\n      <td>[Historical, Samurai]</td>\n      <td>Seinen</td>\n      <td>Morning</td>\n      <td>Inoue, Takehiko (Story &amp; Art), Yoshikawa, Eiji...</td>\n      <td>104</td>\n      <td>[93, 9, 2]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>One Piece</td>\n      <td>9.22</td>\n      <td>392811</td>\n      <td>4</td>\n      <td>4</td>\n      <td>642,620</td>\n      <td>119,974</td>\n      <td>Manga</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Publishing</td>\n      <td>Jul  22, 1997 to ?</td>\n      <td>[Action, Adventure, Fantasy]</td>\n      <td>[]</td>\n      <td>Shounen</td>\n      <td>Shounen Jump (Weekly)</td>\n      <td>Oda, Eiichiro (Story &amp; Art)</td>\n      <td>231</td>\n      <td>[190, 21, 20]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Monster</td>\n      <td>9.16</td>\n      <td>104327</td>\n      <td>5</td>\n      <td>29</td>\n      <td>258,581</td>\n      <td>22,008</td>\n      <td>Manga</td>\n      <td>18</td>\n      <td>162</td>\n      <td>Finished</td>\n      <td>Dec  5, 1994 to Dec  20, 2001</td>\n      <td>[Award Winning, Drama, Mystery]</td>\n      <td>[Adult Cast, Psychological]</td>\n      <td>Seinen</td>\n      <td>Big Comic Original</td>\n      <td>Urasawa, Naoki (Story &amp; Art)</td>\n      <td>86</td>\n      <td>[69, 11, 6]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"df.to_csv('/kaggle/working/raw_manga_1.csv', encoding='utf-8-sig', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:39:01.046443Z","iopub.execute_input":"2024-11-17T10:39:01.047459Z","iopub.status.idle":"2024-11-17T10:39:01.188096Z","shell.execute_reply.started":"2024-11-17T10:39:01.047399Z","shell.execute_reply":"2024-11-17T10:39:01.186851Z"}},"outputs":[],"execution_count":25}]}